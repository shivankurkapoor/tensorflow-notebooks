{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext Cython\n"
     ]
    }
   ],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "       filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "       print(statinfo.st_size)\n",
    "       raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        name = f.namelist()[0]\n",
    "        data = tf.compat.as_str(f.read(name))\n",
    "    return data\n",
    "text = read_data(filename)\n",
    "print(\"Data size %s\" %(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "#Create a small validation set\n",
    "\n",
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character  ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print ('Unexpected character ', char)\n",
    "        return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "\n",
      "\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "\n",
      "\n",
      "[' a']\n",
      "\n",
      "\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size),dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"Generates the next array of batches from the data. The array consists of the \n",
    "        last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn the 1-hot vector encodings or probability distribution over the possible\n",
    "    characters back into its most likely character\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of 2batches into their most likely string representation\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "print(batches2string(train_batches.next()))\n",
    "print('\\n')\n",
    "print(batches2string(train_batches.next()))\n",
    "print('\\n')\n",
    "print(batches2string(valid_batches.next()))\n",
    "print('\\n')\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log prob of the true label in the predicted batch\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions)))/ labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one elem from the distribution assumed to be an array of normalized probs\"\"\"\n",
    "    r = random.uniform(0,1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random columns of probs\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b,1)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    #Parameters\n",
    "    #Input gate: input, previous output, and bias\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    #Forget gate: input, previous ouput, and bias\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    #Memory cell: input, state and bias\n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    #Output gate: input, previous output and bias\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    #Variables saving state across unrolling\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    #Classifier weights and biases\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    #Definition of the cell computation\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    #Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(\n",
    "                tf.float32,\n",
    "                shape=[batch_size, vocabulary_size]\n",
    "            )\n",
    "        )\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:] #labels are inputs shifted by one time step\n",
    "    \n",
    "    #Unrolled LSTM loop\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "    \n",
    "    #State savings across unrollings\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                 saved_state.assign(state)]):\n",
    "        #Classifier\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(0, train_labels),\n",
    "                logits=logits\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    #Optimizer\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True\n",
    "    )\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v),\n",
    "        global_step=global_step\n",
    "    )\n",
    "    \n",
    "    #Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, \n",
    "        saved_sample_output, \n",
    "        saved_sample_state\n",
    "    )\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "def exec_graph(graph):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print('Initialized')\n",
    "        mean_loss = 0\n",
    "        for step in range(num_steps):\n",
    "            batches = train_batches.next()\n",
    "            feed_dict = dict()\n",
    "            for i in range(num_unrollings + 1):\n",
    "                feed_dict[train_data[i]] = batches[i]\n",
    "            _, l, predictions, lr = session.run(\n",
    "                [optimizer, loss, train_prediction, learning_rate], \n",
    "                feed_dict=feed_dict\n",
    "            )\n",
    "            mean_loss += l\n",
    "            # Mean loss the loss of last num_unrolling + 1 batches\n",
    "            if step % summary_frequency == 0:\n",
    "                if step > 0:\n",
    "                    mean_loss = mean_loss / summary_frequency\n",
    "                print(\n",
    "                    'Average loss at step %d: %f learningrate: %f' % (step, mean_loss, lr)\n",
    "                )\n",
    "                mean_loss = 0\n",
    "                labels = np.concatenate(list(batches[1:]))\n",
    "                #Perplexity should decrease\n",
    "                print('Minibatch perplexity: %.2f' % (\n",
    "                        np.exp(logprob(predictions, labels))\n",
    "                    ))\n",
    "                if step % (summary_frequency * 10) == 0:\n",
    "                    print('=' * 80)\n",
    "                    for _ in range(5):\n",
    "                        feed = sample(random_distribution())\n",
    "                        sentence = characters(feed)[0]\n",
    "                        reset_sample_state.run()\n",
    "                        for _ in range(79):\n",
    "                            prediction = sample_prediction.eval({sample_input : feed})\n",
    "                            feed = sample(prediction)\n",
    "                            sentence += characters(feed)[0]\n",
    "                        print(sentence)\n",
    "                    print('=' * 80)\n",
    "                # Measure validation set perplexity.\n",
    "                reset_sample_state.run()\n",
    "                valid_logprob = 0\n",
    "                for _ in range(valid_size):\n",
    "                    b = valid_batches.next()\n",
    "                    predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                    valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "                print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                    valid_logprob / valid_size)))\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 26.97\n",
      "================================================================================\n",
      "glz  kmpskpbdz izihpwhqento ik gexg hk wvoeaaan dovsw fa jtfb btfrq tmte ykdasts\n",
      "fe x thxvmjpfrj eyrrj jkezxuoiethx djvkl  b u bcqu finxokei tgast   coizv tzekao\n",
      "ckvq exseeqogaeo ln nvyi  telaomdsreey o  awywgnj tfeue jwec xttesagglmdsx  jvtg\n",
      "yuqa inll eucgekwcuhr doed  m egmdnef umdl xj z ahb wur mymgdi ggrivegfewsz drcc\n",
      "egyojahlcb n hxdetigtl kihdue ye gpkzen vythlgcingemcnxvmt h n lhauixn di liolkt\n",
      "================================================================================\n",
      "Validation set perplexity: 20.13\n",
      "Average loss at step 100: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 10.49\n",
      "Validation set perplexity: 10.49\n",
      "Average loss at step 200: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 8.40\n",
      "Validation set perplexity: 8.84\n",
      "Average loss at step 300: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 8.09\n",
      "Average loss at step 400: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 7.92\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 500: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 600: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 700: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 800: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 7.04\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 900: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 1000: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "================================================================================\n",
      "unidar v carrasanisipht rldeoved of air pooltver canenging ba can sphorectind th\n",
      "ves or grish heieh this of natuill vanys seckssion flangudy the reutlinc if heat\n",
      "in secifits of ounows controos m imboriry amen inugins wab jusupon dires froht f\n",
      "hon conked that eshptuction is to zear in gist faver of absopy prepberion polic \n",
      "zectomy isthoun his new the bethod maked un a stranns cough frequrys sturch s t \n",
      "================================================================================\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 1100: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 1200: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 1300: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 1400: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1500: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1600: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 1700: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1800: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1900: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2000: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "x whs inspanities one four seven three a west anot used alse many absecce butill\n",
      "s will germent celliak of delatory and paystem nousted consticies fooded of acti\n",
      "x the artornic aftherell demactame and breedan in the issums force national have\n",
      "ger to long thos its poveved at monifimution of far and or lance ided pales ant \n",
      "ver banlex there metented plabe colition of arains aystact in the sences to are \n",
      "================================================================================\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2100: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2200: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2300: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2400: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2500: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2600: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2700: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2800: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2900: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3000: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "================================================================================\n",
      "ction priu lemaz of the toaphation shister a mostle high testents froted knneanu\n",
      "ralloses that the other the gramma to english ble groweling encencella russies t\n",
      "thar time and efricands the utob two one is play a and shate posist duiges of th\n",
      "uson as remicily tro use to of strue was political and yernethy f book matined t\n",
      "gemed itrunnoteal a planet a boye year verd the what ord as used use smazes enat\n",
      "================================================================================\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3100: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3200: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3300: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3400: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3500: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3600: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 3700: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3800: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3900: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 4.16\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4000: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "================================================================================\n",
      "ses to periofacles from armit peed the lessing among pare injuses of bearly to f\n",
      "s was too frainve of he specizenthid pradan fusters marishie kami z bey many oft\n",
      "ness beriomal gobering loading one hisa with the lead of whether one one seven z\n",
      "tsence mispasens epilepsy tearle not firse ranger faciman bedguge seise equalize\n",
      "ve posithy prislaw arract z the custon rikhrassion handprading when plafe and fa\n",
      "================================================================================\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4100: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4200: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4300: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4400: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4500: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4600: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4700: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4800: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4900: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5000: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "================================================================================\n",
      "phice was exbalion the precus follows known example of coliginnay bodghol arabis\n",
      "y was their valizef an and of beilic all the haw one nine nine eight six zero we\n",
      "ver informate wilrituremil was hole manistater the thon nutber the mattection of\n",
      "jartably reduge of the rilemo carlames revortang is a deople which toatev to are\n",
      "jeaders included to maps canarepitions propress of throal stifes four seven m le\n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5100: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5200: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5300: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5400: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5500: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5600: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5700: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5800: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5900: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6000: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "================================================================================\n",
      "uss souncy jungs arribated from three for the necausan sprece the was not in new\n",
      "farnes in the names manys tendest of fo variama and a acrass he i polkoom blazaa\n",
      "cial one nine one six thie at empire with and simans eventaa layining a hamoud w\n",
      "up waters and nide colinud s lazors and writing iopoeth may toda asspitations fo\n",
      "zerot f comany to war is are ins jods own its ported tenms bevelel plane cloody \n",
      "================================================================================\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6100: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6200: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6300: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6400: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6500: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6600: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6700: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6800: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6900: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 7000: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      "acian was culture recent ecolic bown wild cambor and emmerint asian hanualises b\n",
      "ques exciple g giventany mathls beataped and why auroposyes everysion navigaus b\n",
      "hiasozm ment apccurs the stoble useing womendius was somemsts celtpprants lannas\n",
      "thicle k is focudying moveoma have engrodically whe history fanters sivingy the \n",
      "es to unies including unscually occurbed by in the viching only cithane with als\n",
      "================================================================================\n",
      "Validation set perplexity: 4.50\n",
      "CPU times: user 3min 53s, sys: 1min 6s, total: 5min\n",
      "Wall time: 2min 33s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 - LSTM with optimized matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    #Parameters\n",
    "    #Big matrices\n",
    "    ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, 4*num_nodes], -0.1, 0.1))\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "    ifcob = tf.Variable(tf.truncated_normal([1, 4*num_nodes], -0.1, 0.1))\n",
    "    \n",
    "    #Variables saving state across unrolling\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    #Classifier weights and biases\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    #Definition of the cell computation\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        all_gates = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "        input_gate = tf.sigmoid(all_gates[:, 0:num_nodes])\n",
    "        forget_gate = tf.sigmoid(all_gates[:, num_nodes:2*num_nodes])\n",
    "        update = all_gates[:, 2*num_nodes:3*num_nodes]\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(all_gates[:, 3*num_nodes:])\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    #Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(\n",
    "                tf.float32,\n",
    "                shape=[batch_size, vocabulary_size]\n",
    "            )\n",
    "        )\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:] #labels are inputs shifted by one time step\n",
    "    \n",
    "    #Unrolled LSTM loop\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "    \n",
    "    #State savings across unrollings\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                 saved_state.assign(state)]):\n",
    "        #Classifier\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(0, train_labels),\n",
    "                logits=logits\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    #Optimizer\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True\n",
    "    )\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss)) #returns list of gradients, variables\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v),\n",
    "        global_step=global_step\n",
    "    )\n",
    "    \n",
    "    #Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, \n",
    "        saved_sample_output, \n",
    "        saved_sample_state\n",
    "    )\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 26.80\n",
      "================================================================================\n",
      "bpez caoqonrzaan lyeyd crcle ytqaazvooxyq  y dwioer ldaosehld b okenhtwpzd pnx w\n",
      "ut rywmoitzkc vtlktour n  tuso vj lj n tlejr ulmev itbnrvk sra   gnet oaesoz  jd\n",
      "satnnpvh tsoatdqt  whsq oqlwrsq wmjeo pe otesmhqxap m yl ss    ls nwqoq itwosbn \n",
      "ug jaefweak s ur oxto rcmcnh istpdebn ohsn iwnwtuxqeuselr ci eihuglrimx cl od dr\n",
      "acerejk pfog nrvt wts heecapedt erfit odpd  ssv m  wp r tpmhkdauo nrenhjlnmytclk\n",
      "================================================================================\n",
      "Validation set perplexity: 19.62\n",
      "Average loss at step 100: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 12.03\n",
      "Validation set perplexity: 10.94\n",
      "Average loss at step 200: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 8.47\n",
      "Validation set perplexity: 9.05\n",
      "Average loss at step 300: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 8.21\n",
      "Average loss at step 400: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 6.89\n",
      "Validation set perplexity: 8.02\n",
      "Average loss at step 500: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 600: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 700: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 800: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 900: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 1000: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "================================================================================\n",
      "logenting isshercct e prazer losk dagent of proplenser of manremon these demerse\n",
      "uloonal threen e one comin domasti ar domue otfermigal one nine zith a reyelitin\n",
      "nowician idingrocist had one one five one one never tabyramous ayuolyting in tha\n",
      "y mity and pospropubure on tnomopristan it encentigute is alorades at by firm sc\n",
      "pert and the garcaby eacleope phililiticed bayk one zige beyt innored of etial u\n",
      "================================================================================\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 1100: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 1200: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 1300: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 1400: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 1500: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 1600: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1700: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1800: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1900: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 2000: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "================================================================================\n",
      "valon propeck is lost if one five deree trosshuzols necteding afome is edg hista\n",
      "ther of and releriase one nine one juh diph when borts reever hohib kisporescall\n",
      "st six four manay the  etecper me sopeives resect unteen hriture one two zero by\n",
      "stical tister tore omeriteds alineg the machine whyseven beentianba that bases t\n",
      "fore waring often inclubate with otherber the due the trate they soce peared to \n",
      "================================================================================\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 2100: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 2200: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2300: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2400: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2500: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2600: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2700: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2800: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2900: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3000: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "================================================================================\n",
      "ul poponies of the exolmph originians including qurial an eights low basinnisis \n",
      "mannmenciuate n u ddemps with been wark are ba pogoies gunst syphard to on it as\n",
      "ulty a marie wast to char of gheens aute prozerging and a for werphried windows \n",
      "ing is a files a rund wikemy has enquens of is recapered ktill cen jod solaced t\n",
      "ring in isemin roup with paid undolic of europy norns dovine to the kall the vir\n",
      "================================================================================\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3100: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 3200: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3300: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3400: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3500: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3600: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3700: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3800: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3900: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4000: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "================================================================================\n",
      "vil eishiman philobalth are streekime united ierall usthrial alsousing of the fo\n",
      "macter and thzw in one of demasent unligion dits though to handuck infeito three\n",
      "hills correnoblom harrism public allowing has its ussoly its levivedns and tomga\n",
      "hangtists institutions foves of ving the times from the up one seven culilic s t\n",
      "nists of afcerves in the newarders aput howev windows of his number with this th\n",
      "================================================================================\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4100: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 4200: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4300: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 4400: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4500: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 4600: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 4700: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4800: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4900: 1.000000 learningrate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 5000: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 5.45\n",
      "================================================================================\n",
      "chmbba of derratury union one three one site and topto prime uberroto with tho y\n",
      "y be copiretal arrial as the deleminers was wexptitive were one five sonns proan\n",
      "x the ditactified is he one one three fave release use whichwouncists a sing sip\n",
      "ffutiot charica byelchelang years imbn by a the form per prime recumpled firsed \n",
      "h shewing thes way real picentives the american many youth of stagentop nevere n\n",
      "================================================================================\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5100: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5200: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5300: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5400: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5500: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5600: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5700: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 3.90\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5800: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5900: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6000: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "================================================================================\n",
      "v this fide chure was include sabra got of gistory pathant americandausi prilume\n",
      "reschess frowsion heraxim the feature belien ards monge lleking in near is the g\n",
      "retwork canters one nine four usporch suicn for visition these parehed proputed \n",
      "n cormal duse of the arvarion carrert on one five mistoring theyly stanne of the\n",
      "jow the v in carsomal a to ithiv example underpuncies of lebands is not th ents \n",
      "================================================================================\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6100: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6200: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6300: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6400: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6500: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6600: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6700: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6800: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6900: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 7000: 1.000000 learningrate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "================================================================================\n",
      "nef known a creeghtotics to the one nine seven one thregeen there was spiqua was\n",
      "maral river required india two three two sucuals pases no for thome with most fi\n",
      "ade than franchly continuanical he ins sequatation reporty dealy carse was b aft\n",
      "ks bring constructures feoult and the name is the des typinalit by sucksov muctw\n",
      "an leaver about the one nine nine three three five seven eight seven side five m\n",
      "================================================================================\n",
      "Validation set perplexity: 4.48\n",
      "CPU times: user 4min 33s, sys: 37.9 s, total: 5min 11s\n",
      "Wall time: 2min 29s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.<br>\n",
    "a. Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.<br>\n",
    "b. Write a bigram-based LSTM, modeled on the character LSTM above.<br>\n",
    "c. Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "bigram_size = vocabulary_size * vocabulary_size\n",
    "\n",
    "#output embeddings (IDs) instead of 1-hot vector\n",
    "class BigramEmbeddingBatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = list()\n",
    "        for b in range(self._batch_size):\n",
    "            first_char = self._text[self._cursor[b]]\n",
    "            if self._cursor[b] + 1 == self._text_size:\n",
    "                second_char = ' '\n",
    "            else:\n",
    "                second_char = self._text[self._cursor[b] + 1]\n",
    "            batch.append(char2id(first_char) * vocabulary_size + char2id(second_char))\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "    \n",
    "def bigram_characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return ['({0},{1})'.format(id2char(c//vocabulary_size), id2char(c % vocabulary_size))\n",
    "            for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def bigram_first_characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c//vocabulary_size)\n",
    "            for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def bigram_batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, bigram_characters(b))]\n",
    "    return s\n",
    "\n",
    "\n",
    "bigram_embed_train_batches = BigramEmbeddingBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "bigram_embed_valid_batches = BigramEmbeddingBatchGenerator(valid_text, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bigram_sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, bigram_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def bigram_random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, bigram_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "with graph.as_default():\n",
    "    # Parameters:\n",
    "    # big matrix\n",
    "    ifcox = tf.Variable(tf.truncated_normal([bigram_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    ifcob = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, bigram_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([bigram_size]))\n",
    "    \n",
    "    #Defining the LSTM cell\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        all_gates = tf.nn.embedding_lookup(ifcox, i) + tf.matmul(o, ifcom) + ifcob\n",
    "        input_gate = tf.sigmoid(all_gates[:, 0:num_nodes])\n",
    "        forget_gate = tf.sigmoid(all_gates[:, num_nodes:2*num_nodes])\n",
    "        update = all_gates[:, 2*num_nodes:3*num_nodes]\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(all_gates[:, 3*num_nodes:])\n",
    "        \n",
    "        return output_gate * tf.tanh(state), state\n",
    "       \n",
    "    \n",
    "    #Input data\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.int64, shape=[batch_size])\n",
    "        )\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:] #lables are shifted by one timestep\n",
    "    \n",
    "    #Unrolled LSTM loop\n",
    "    outputs = list()\n",
    "    output = saved_output \n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "    \n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                 saved_state.assign(state)]):\n",
    "        #Classifier\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            #sparse_softmax because we are not using 1 hot vector in labels\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits,\n",
    "                tf.concat(0, train_labels)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    #Optimizer\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True\n",
    "    )\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    #Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int64, shape=[1])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, \n",
    "        saved_sample_output, \n",
    "        saved_sample_state\n",
    "    )\n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 10001\n",
    "summary_frequency = 200\n",
    "\n",
    "def exec_graph_bigram_embed(graph):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print('Initialized')\n",
    "        mean_loss = 0\n",
    "        for step in range(num_steps):\n",
    "            batches = bigram_embed_train_batches.next()\n",
    "            feed_dict = dict()\n",
    "            for i in range(num_unrollings + 1):\n",
    "                feed_dict[train_data[i]] = batches[i]\n",
    "            _, l, predictions, lr = session.run(\n",
    "                [optimizer, loss, train_prediction, learning_rate],\n",
    "                feed_dict = feed_dict\n",
    "            )\n",
    "            mean_loss += l\n",
    "            if step % summary_frequency == 0:\n",
    "                if step > 0:\n",
    "                    mean_loss = mean_loss / summary_frequency\n",
    "                # The mean loss is an estimate of the loss over the last few batches.\n",
    "                print(\n",
    "                    'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr)\n",
    "                )\n",
    "                mean_loss = 0\n",
    "                labels = np.concatenate(list(batches)[1:])\n",
    "                # convert to one-hot-encodings\n",
    "                noembed_labels = np.zeros(predictions.shape)\n",
    "                for i, j in enumerate(labels):\n",
    "                    noembed_labels[i,j] = 1.0\n",
    "                print('Minibatch perplexity: %.2f' % float(\n",
    "                          np.exp(logprob(predictions, noembed_labels\n",
    "                                )\n",
    "                        )\n",
    "                    ) \n",
    "                )\n",
    "                if step % (summary_frequency * 10) == 0:\n",
    "                    # Generate some samples.\n",
    "                    print('=' * 80)\n",
    "                    for _ in range(5):\n",
    "                        feed = bigram_sample(bigram_random_distribution())\n",
    "                        bigram_sentence = bigram_characters(feed)[0]\n",
    "                        sentence = bigram_first_characters(feed)[0]\n",
    "                        # convert to embedding\n",
    "                        feed = [np.argmax(feed)]\n",
    "                        reset_sample_state.run()\n",
    "                        for _ in range(79):\n",
    "                            prediction = sample_prediction.eval({sample_input: feed})\n",
    "                            feed = bigram_sample(prediction)\n",
    "                            bigram_sentence += bigram_characters(feed)[0]\n",
    "                            sentence += bigram_first_characters(feed)[0]\n",
    "                            feed = [np.argmax(feed)]\n",
    "                        print('bigrams:', bigram_sentence)\n",
    "                        print('chars:', sentence)\n",
    "                    print('=' * 80)\n",
    "                    # Measure validation set perplexity.\n",
    "                    reset_sample_state.run()\n",
    "                    valid_logprob = 0\n",
    "                    for _ in range(valid_size):\n",
    "                        b = bigram_embed_valid_batches.next()\n",
    "                        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                        labels = np.zeros((1, bigram_size))\n",
    "                        labels[0, b[1]] = 1.0\n",
    "                        valid_logprob = valid_logprob + logprob(predictions, labels)\n",
    "                    print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                      valid_logprob / valid_size)))\n",
    "                \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 729.41\n",
      "================================================================================\n",
      "bigrams: (d,o)(z,h)(m,j)(k,h)(r,h)(n,c)(i,q)(i,k)(m,q)(e,n)(r,e)(g,d)(a,u)(s,g)(e, )(z,h)(c,x)(p,h)(l,p)( ,o)(o,v)(r,t)(u,t)(u,t)(j,s)(j,c)(y,n)(m,r)(z,d)(a,l)(j,i)(k,y)(v,f)(e,j)( ,w)(g,h)(r,x)(l,o)(f,v)(z,c)(w,k)(o,h)(z,c)(o,e)(j,j)(k,g)(g,q)(m,q)(w,c)(p,u)(b,n)( ,e)(t,g)(c,e)(k,s)(f,z)(n,w)(o,t)(b,p)(v,n)(t,g)(c,n)(p,q)(y,h)( ,g)(m,v)( ,r)(d, )(h,b)(w,m)(z,t)( ,t)(l,o)(o,k)(e,l)(l,x)(b,y)(x,o)(k,v)(j,j)\n",
      "chars: dzmkrniimergasezcpl oruujjymzajkve grlfzwozojkgmwpb tckfnobvtcpy m dhwz loelbxkj\n",
      "bigrams: (e,f)(y,o)(x,y)(l,t)(i,v)(r,o)(j, )(s,g)(g,n)(g,s)(n,d)(g,l)(v,t)(t,t)(z,e)(o,s)(r,f)(g,g)(f,x)(b,k)(h,q)(k,n)(d, )(r,u)(i,z)(v,z)(x,x)(a,b)(i,v)(y,b)(h, )(m,g)(b,f)(f,g)( ,h)(g,h)(t, )(q,s)(h,i)(i,c)(o,r)(n,c)(v,l)(s, )(w,g)(z,q)(k,s)(x,m)(h,y)(z,y)(b,g)(l,f)(c,r)(j,o)(j,d)(c,m)(z,d)(y,y)(t,d)(y,b)(q,a)(h,o)(d,m)(g,n)(z,f)(m,v)(z,q)(n,u)(q,w)(r, )(h,a)(i,l)(i,m)(b,t)(k,c)(b,z)(j,s)(z,g)(j,k)(k,i)\n",
      "chars: eyxlirjsggngvtzorgfbhkdrivxaiyhmbf gtqhionvswzkxhzblcjjczytyqhdgzmznqrhiibkbjzjk\n",
      "bigrams: (c,l)(d,h)(r,f)(v,v)(o,p)(f,b)(d,b)(e,w)(j,o)(z,w)(g,t)(o,g)( ,k)(a,f)(g,a)(k,n)(m,n)(e,y)(b,j)(g,c)(o,e)(b,e)(o,v)(t,h)(c,m)(x,i)(h,e)(d,q)(y,d)(d,u)(z,z)(c,o)(y,d)(f,z)(b,r)(u,b)(a,v)(l,q)(k,w)(c,s)(s,g)(p,t)(k,d)(t,a)(t,u)(h,e)(t,n)(s,f)(e,e)(c,x)(g,r)(a,g)(v,h)( ,x)(x,q)(m,s)(b,i)(c,a)(a,v)( ,c)(d,e)(z,k)(q,r)(i,e)(g,a)(d,w)(m,f)( ,p)(u,x)(u,y)( ,u)(a,x)(j,p)(m, )(r,p)(a,t)(y,o)(e,l)(j,n)(b,u)\n",
      "chars: cdrvofdejzgo agkmebgobotcxhdydzcyfbualkcspktthtsecgav xmbca dzqigdm uu ajmrayejb\n",
      "bigrams: ( ,t)(m,g)( ,x)( ,o)(g,p)(c,s)(j,u)(x,i)(w,d)(v,z)(v,m)(f,o)(u,h)(q,w)(w,u)(x,m)( ,v)(o,w)(h,t)(y,e)(d,j)(l,d)(j,o)(e,m)(i,c)(v,d)(a, )(t,v)( ,l)(y,b)(b,r)(o,d)(p,b)(u,i)(a,h)(c,s)(k,f)(l,t)(q,b)(h,l)(e,q)(w, )(u,g)(f,f)(y,d)(l,p)(t,r)(r,p)(d,o)(e, )(t,g)( ,c)(x,z)(x,n)(e,t)(y,p)(j,r)(i,c)(m,t)(i,v)(g,d)(j,k)(q,u)(i,h)(b,g)(o,k)(b,o)(f,o)(p,x)(c,r)( ,n)(h,n)(z,a)(d,b)(b,w)(h,g)(a,z)(q,m)(n,x)(d,j)\n",
      "chars:  m  gcjxwvvfuqwx ohydljeivat ybopuacklqhewufyltrdet xxeyjimigjqibobfpc hzdbhaqnd\n",
      "bigrams: (b,c)(w,k)(k,v)(n,e)(c,m)(r,q)(h,x)(g,y)(h,s)(l,t)(y,o)(k, )(s,y)(e,n)(e,r)(y,t)(a,q)(p,r)(i,h)(n,w)(j,n)(x,o)(d,k)(n,b)( ,r)(m,s)(k,g)(j,r)(d,g)(w,q)(n,k)(b,n)(d, )(n,f)(g,g)(b,d)(a,x)(u,y)(e,x)(p,z)(v,p)(b,r)(a, )(b,o)(o,i)(b,i)(z,w)( , )(y,d)(g,z)(h,h)(r,l)(v,r)(k,s)(b,l)( ,t)(r,h)(j,y)(d,t)(p,z)(r,i)(k,k)(o,u)(k,n)( ,x)(e,o)(c,o)(e,m)(t,l)(x,g)(z,r)(c,o)(y,e)(n,a)(w,e)(z,p)(t,x)(k,w)(k,d)(a,h)\n",
      "chars: bwkncrhghlykseeyapinjxdn mkjdwnbdngbauepvbabobz yghrvkb rjdprkok ecetxzcynwztkka\n",
      "================================================================================\n",
      "Validation set perplexity: 679.20\n",
      "Average loss at step 200: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.32\n",
      "Average loss at step 400: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.20\n",
      "Average loss at step 600: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.58\n",
      "Average loss at step 800: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.38\n",
      "Average loss at step 1000: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Average loss at step 1200: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Average loss at step 1400: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Average loss at step 1600: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Average loss at step 1800: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Average loss at step 2000: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "================================================================================\n",
      "bigrams: (o,c)(c,u)(u,l)(l,a)(a,t)(t,e)(e, )( ,b)(b,i)(i,a)(a,d)(d,l)(l,e)(e,t)(t,l)(l,e)(e,s)(s, )( ,d)(d,a)(a,t)(t,a)(a,l)(l,a)(a,r)(r, )( ,s)(s,o)(o,m)(m,s)(s, )( ,f)(f,u)(u,l)(l,l)(l, )( ,a)(a, )( ,o)(o,r)(r,b)(b,i)(i,t)(t,y)(y, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,t)(t,h)(h,e)(e, )( ,s)(s,o)(o, )( ,t)(t,h)(h,e)(e, )( ,g)(g,r)(r,o)(o,u)(u,p)(p, )( ,l)(l,o)(o,w)(w,e)(e,d)(d, )( ,a)(a,l)(l,s)(s,o)(o, )( ,c)\n",
      "chars: oculate biadletles datalar soms full a orbity eight the so the group lowed also \n",
      "bigrams: (r,d)(d,e)(e,r)(r,y)(y, )( ,h)(h,a)(a,m)(m,b)(b,u)(u,r)(r, )( ,p)(p,r)(r,o)(o,l)(l, )( ,b)(b,e)(e,s)(s,t)(t, )( ,s)(s,t)(t,y)(y, )( ,p)(p,o)(o,s)(s,i)(i,z)(z,e)(e,d)(d, )( ,o)(o,f)(f, )( ,h)(h,i)(i,g)(g,h)(h,t)(t,e)(e,i)(i,v)(v,e)(e, )( ,o)(o,n)(n,e)(e, )( ,o)(o,f)(f, )( ,m)(m,a)(a,l)(l,d)(d, )( ,r)(r,e)(e,f)(f,e)(e,r)(r,m)(m,e)(e,d)(d, )( ,s)(s,i)(i,x)(x, )( ,f)(f,i)(i,v)(v,e)(e, )( ,t)(t,w)(w,o)\n",
      "chars: rdery hambur prol best sty posized of highteive one of mald refermed six five tw\n",
      "bigrams: (q,y)(d, )( ,a)(a, )( ,f)(f,i)(i,r)(r,s)(s,t)(t, )( ,b)(b,e)(e,a)(a, )( ,g)(g,a)(a,c)(c,i)(i,z)(z,e)(e,d)(d, )( ,l)(l,i)(i,g)(g,h)(u,e)(e,s)(s, )( ,u)(u,s)(s,e)(e,s)(s, )( ,a)(a, )( ,s)(s,t)(t,a)(a,b)(b,l)(l,e)(e,d)(d, )( ,f)(f,o)(o,r)(r, )( ,a)(a,p)(p,p)(p,l)(l,a)(a,r)(r,a)(a, )( ,w)(w,a)(a,s)(s, )( ,o)(o,r)(r,t)(t,e)(e,n)(n,i)(i,n)(n,e)(e, )( ,b)(b,a)(a,c)(c,k)(t,r)(r,a)(a,c)(c,r)(r,e)(e,s)(s, )\n",
      "chars: qd a first bea gacized ligues uses a stabled for applara was ortenine bactracres\n",
      "bigrams: (z, )( ,v)(v,a)(a,r)(r,y)(y, )( ,n)(n,e)(e,t)(t,w)(w,o)(o, )( ,r)(r,e)(e,l)(l,a)(a,n)(n,d)(d, )( ,s)(s,t)(t,o)(o,v)(v,e)(e,r)(r, )( ,o)(o,h)(h,i)(i,s)(s,t)(t,o)(o,r)(r, )( ,c)(c,e)(e,n)(n,t)(t,u)(u,a)(a,l)(l, )( ,o)(o,f)(f, )( ,c)(c,l)(l,i)(i,m)(m,a)(a,t)(t,i)(i,p)(p,l)(l,e)(e, )( ,l)(l,i)(i,k)(k,i)(i,t)(t, )( ,j)(j,a)(a,v)(v,a)(a, )( ,a)(a,i)(i,n)(n,e)(e,t)(t,h)(h,e)(e,r)(r,n)(n, )( ,s)(s,m)(m,a)\n",
      "chars: z vary netwo reland stover ohistor centual of climatiple likit java ainethern sm\n",
      "bigrams: (t,j)(z,a)(a, )( ,s)(s,t)(t,e)(e,r)(r,m)(m,s)(s, )( ,l)(l,e)(e,a)(a,d)(d,a)(a,b)(b,i)(i,l)(l,e)(e, )( ,w)(w,a)(a,s)(s, )( ,t)(t,h)(h,e)(e, )( ,o)(o,f)(f, )( ,b)(b,y)(y, )( ,b)(b,u)(u,r)(r,n)(n, )( ,e)(e,m)(m,c)(c,h)(h,e)(e,t)(t,a)(a,l)(l,l)(l,y)(y, )( ,t)(t,h)(h,a)(a,t)(t, )( ,d)(d, )( ,o)(o,b)(b,r)(r,a)(a,m)(m,e)(e,d)(d, )( ,t)(t,h)(h,e)(e, )( ,f)(f,o)(o,r)(r, )( ,h)(h,a)(a,l)(l,d)(d, )( ,i)(i,n)\n",
      "chars: tza sterms leadabile was the of by burn emchetally that d obramed the for hald i\n",
      "================================================================================\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 2200: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Average loss at step 2400: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Average loss at step 2600: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Average loss at step 2800: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Average loss at step 3000: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Average loss at step 3200: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Average loss at step 3400: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Average loss at step 3600: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Average loss at step 3800: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Average loss at step 4000: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "================================================================================\n",
      "bigrams: (a,k)(k,i)(i,n)(n,g)(g, )( ,i)(i,m)(m,p)(p,r)(r,o)(o,s)(s,e)(e,s)(s, )( ,m)(m,e)(e,t)(t,r)(r,o)(o,l)(l,a)(a,y)(y,e)(e,r)(r,d)(d, )( ,t)(t,h)(h,e)(e, )( ,p)(p,e)(e,r)(r, )( ,t)(t,o)(o,o)(o,d)(d,s)(s, )( ,i)(i,n)(n,d)(d,u)(u,r)(r,y)(y, )( ,u)(u,n)(n,i)(i,v)(v,e)(e,r)(r,s)(s,i)(i,t)(t,i)(i,o)(o,n)(n, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,b)(b,r)(r,i)(i,s)(s,m)(m,o)(o,n)(n,s)(s, )( ,s)(s,e)(e,r)(r,v)\n",
      "chars: aking improses metrolayerd the per toods indury universition of the brismons ser\n",
      "bigrams: (p,q)(s,i)(i,s)(s, )( ,o)(o,f)(f, )( ,e)(e,p)(p,t)(t,e)(e,r)(r,f)(f,e)(e, )( ,d)(d,w)(w,i)(i,t)(t,h)(h,e)(e,r)(r, )( ,j)(j,u)(u,s)(s,u)(u,a)(a,l)(l, )( ,v)(v,y)(y,i)(i,n)(n,g)(g, )( ,k)(k,i)(i,n)(n,g)(g,t)(t,i)(i,v)(v,e)(e, )( ,o)(o,r)(r, )( ,a)(a,n)(n,n)(n,u)(u,m)(m,b)(b,e)(e,r)(r, )( ,o)(o,f)(f, )( ,a)(a,r)(r,e)(e, )( ,r)(r,e)(e,c)(c,o)(o,r)(r,d)(d,i)(i,n)(n,g)(g, )( ,n)(n,o)(o,t)(t, )( ,h)(h,a)\n",
      "chars: psis of epterfe dwither jusual vying kingtive or annumber of are recording not h\n",
      "bigrams: (l,r)(r,a)(a,t)(t,e)(e,d)(d, )( ,n)(n,e)(e,a)(a,r)(r, )( ,a)(a,u)(u,g)(g,h)(h, )( ,o)(o,n)(n,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,r)(r,e)(e,b)(f,u)(u,n)(n,t)(t,e)(e,n)(n,d)(d,e)(e,d)(d, )( ,a)(a, )( ,w)(w,e)(e, )( ,r)(r,e)(e,f)(f,e)(e,r)(r,e)(e,n)(n,c)(c,y)(k,l)(l,y)(y,i)(i,n)(n,t)(t, )( ,a)(a,p)(p,p)(p,o)(o,p)(p,e)(e,r)(r,a)(a,t)(t,e)(e,d)(d, )( ,i)(i,t)(t, )( ,i)(i,r)(r,e)(e,l)(l, )( ,t)(t,o)\n",
      "chars: lrated near augh one seven refuntended a we referencklyint appoperated it irel t\n",
      "bigrams: (e,o)(o,u)(u,n)(n,t)(t,a)(a,r)(r,s)(s, )( ,o)(o,r)(r, )( ,p)(p,r)(r,o)(o,t)(t,o)(o,n)(n, )( ,b)(b,y)(y, )( ,l)(l,e)(e,g)(g,a)(a,r)(r, )( ,a)(a,t)(t, )( ,p)(p,d)(d,u)(u,s)(s,e)(e,d)(d, )( ,t)(t,h)(h,e)(e, )( ,b)(b,u)(u,m)(m,b)(l,t)(t, )( ,m)(b,a)(o,r)(r,t)(t,u)(u,r)(r,g)(g, )( ,d)(d,a)(a,y)(y, )( ,t)(t,o)(o, )( ,d)(d,i)(i,s)(s,s)(s,a)(a,p)(p, )( ,a)(a,f)(f,i)(i,l)(l, )( ,w)(w,a)(a,y)(y, )( ,w)(w,a)\n",
      "chars: eountars or proton by legar at pdused the bumlt borturg day to dissap afil way w\n",
      "bigrams: (f,f)(f,i)(i,c)(c,a)(a,t)(t,i)(i,l)(l,a)(a,n)(n, )( ,l)(l,o)(o,r)(r,g)(g,u)(u,b)(b, )( ,t)(t,h)(h,e)(e, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,o)(o,f)(f, )( ,f)(f,i)(i,n)(n,e)(e,s)(s, )( ,t)(t,h)(h,e)(e,s)(s,e)(e, )( ,m)(m,a)(a,s)(s,q)(q,u)(u,e)(e,s)(s, )( ,n)(n,o)(o,w)(w, )( ,e)(e, )( ,f)(f,r)(r,o)(o,m)(m, )( ,t)(t,r)(r,u)(u,t)(t,h)(h,e)(e,s)(s,e)(e, )( ,i)(i,n)(n,t)(t,o)(o, )( ,t)(t,h)\n",
      "chars: fficatilan lorgub the one nine of fines these masques now e from truthese into t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4200: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Average loss at step 4400: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Average loss at step 4600: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Average loss at step 4800: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Average loss at step 5000: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Average loss at step 5200: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Average loss at step 5400: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Average loss at step 5600: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Average loss at step 5800: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Average loss at step 6000: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "================================================================================\n",
      "bigrams: (o,r)(r,e)(e, )( ,h)(h,a)(a,d)(d, )( ,j)(j,o)(o,h)(h,n)(n, )( ,s)(s,o)(o,m)(m,e)(e, )( ,s)(s,i)(i,n)(n,g)(g,l)(l,a)(a,n)(n,i)(i,c)(c,u)(u,l)(l,t)(t,a)(a,n)(n,s)(s,u)(u,s)(s,h)(h,i)(i,n)(n,g)(g, )( ,o)(o,b)(b,j)(j,e)(e,c)(c,t)(t,i)(i,o)(o,n)(n, )( ,a)(a,n)(n,d)(d, )( ,e)(e, )( ,n)(n,a)(a, )( ,a)(a,n)(n,d)(d, )( ,o)(o,t)(t,h)(h,e)(e,n)(n, )( ,e)(e,l)(l,e)(e,c)(c,t)(t, )( ,o)(o,n)(n, )( ,t)(t,h)(h,e)\n",
      "chars: ore had john some singlanicultansushing objection and e na and othen elect on th\n",
      "bigrams: (z,i)(y,i)(i,i)(i, )( ,x)(k,b)(l,e)(e,a)(a,t)(t,a)(a,n)(n, )( ,r)(r,u)(u,l)(l,l)(l, )( ,c)(c,h)(h,e)(e,m)(m,i)(i,s)(s,t)(t,r)(r,y)(y, )( ,o)(o,n)(n,e)(e, )( ,o)(o,f)(f, )( ,m)(m,a)(a,t)(t,t)(t,y)(x,p)(p,r)(r,e)(e,x)(x, )(a,m)(m,p)(p,i)(i,r)(r,i)(i,n)(n,g)(g, )( ,p)(p,r)(r,o)(o,g)(g,r)(r,a)(a,m)(m,m)(m,i)(i,n)(n,g)(g, )( ,s)(s, )( ,d)(d,e)(e,l)(l,i)(i,g)(g,h)(h,t)(t, )( ,s)(s, )( ,s)(s,o)(o,m)(m,e)\n",
      "chars: zyii kleatan rull chemistry one of mattxprexampiring programming s delight s som\n",
      "bigrams: (x,a)(a, )( ,w)(w,i)(i,n)(n,d)(d, )( ,a)(a,g)(g,a)(a,i)(i,r)(r, )( ,p)(p,o)(o,l)(l,i)(i,c)(c,e)(e,d)(d, )( ,s)(s, )( ,l)(l,y)(y,s)(s,t)(t,r)(r,a)(a,i)(i,n)(n, )( ,t)(t,o)(o, )( ,s)(s,o)(o, )( ,t)(t,h)(h,e)(e,r)(r,e)(e, )( ,i)(i,s)(s, )( ,a)(a, )( ,p)(p,a)(a,y)(y, )( ,h)(h,i)(i,s)(s,t)(t,o)(o,r)(r,y)(y, )( ,o)(o,f)(f, )( ,t)(t,h)(h,o)(o,u)(u,g)(g,h)(h, )( ,h)(h,o)(o,w)(w,e)(e,v)( ,c)(c,y)(y, )( ,r)\n",
      "chars: xa wind agair policed s lystrain to so there is a pay history of though howe cy \n",
      "bigrams: (p,a)(a,g)(g,e)(e, )( ,w)(w,h)(h,e)(e,n)(n, )( ,h)(h,a)(a,v)(v,e)(e, )( ,t)(t,w)(w,o)(o, )( ,w)(w,o)(o,u)(u,l)(l,d)(d, )( ,o)(o,n)(n, )( ,o)(o,n)(n,e)(e, )( ,f)(f,i)(i,l)(l,l)(l,o)(o,r)(r, )( ,s)(s,t)(t,a)(a,r)(r,d)(d,s)(s, )( ,t)(t,h)(h,e)(e,i)(i,r)(r, )( ,f)(f,a)(a,m)(m,i)(i,l)(l,y)(y, )( ,o)(o,r)(r, )( ,h)(h,a)(a,s)(s, )( ,v)(v,a)(a,r)(r,e)(e,n)(n,t)(t,s)(s, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,p)\n",
      "chars: page when have two would on one fillor stards their family or has varents eight \n",
      "bigrams: (h,z)(g, )( ,a)(a, )( ,b)(b,e)(e,c)(c,a)(a,t)(t,e)(e,d)(d, )( ,f)(f,o)(o,r)(r,m)(m,i)(i,n)(n,g)(g, )( ,o)(o,f)(f, )( ,s)(s,a)(a,m)(m, )( ,a)(a,l)(l,s)(s,o)(o, )( ,g)(g,r)(r,e)(e,a)(a,t)(t,t)(t,e)(e,r)(r, )( ,o)(o,n)(n, )( ,o)(o,l)(l,d)(d, )( ,p)(p,e)(e,r)(r,u)(u,s)(s, )( ,g)(g,i)(i,s)(s, )( ,p)(p,e)(e,r)(r,f)(f,e)(e,r)(r,t)(t,s)(s, )( ,o)(o,r)(r, )( ,t)(t,e)(e,c)(c,h)(h,n)(n,o)(o,l)(l,o)(o,g)(g,i)\n",
      "chars: hg a becated forming of sam also greatter on old perus gis perferts or technolog\n",
      "================================================================================\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6200: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Average loss at step 6400: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Average loss at step 6600: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Average loss at step 6800: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Average loss at step 7000: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Average loss at step 7200: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.13\n",
      "Average loss at step 7400: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Average loss at step 7600: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Average loss at step 7800: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Average loss at step 8000: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.14\n",
      "================================================================================\n",
      "bigrams: (l,u)(u, )( ,c)(c,l)(l,a)(a,n)(n,e)(e,t)(t, )( ,e)(e,p)(p,i)(i,s)(s,t)(t,u)(u,r)(r,i)(i,n)(n,g)(g, )( ,a)(a,n)(n,d)(d, )( ,t)(t,h)(h,e)(e, )( ,b)(b,i)(i,r)(r,t)(t,h)(h, )( ,o)(o,p)(p,e)(e,r)(r, )( ,g)(g,o)(o,l)(l,e)(e,n)(n,t)(t, )( ,u)(u,n)(n,d)(d,e)(e,r)(r, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,d)(d,i)(i,s)(s,p)(p,h)(h,e)(e,a)(a,t)(t,e)(e,d)(d, )( ,a)(a,r)(r,e)(e, )( ,d)(d,i)(i,g)(g,i)(i,v)(v,e)(e,r)\n",
      "chars: lu clanet episturing and the birth oper golent under three dispheated are digive\n",
      "bigrams: (q,c)(z, )( ,a)(a,r)(r,e)(e,a)(a,r)(r,i)(i,n)(n,g)(g, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,i)(i,x)(x, )( ,a)(a,n)(n,t)(t,e)(e,l)(l,k)(k,a)(a, )( ,s)(s,h)(h,o)(o,w)(w,s)(s, )( ,t)(t,h)(h,e)(e, )( ,b)(b,a)(a,s)(s,e)(e,r)(r,o)(o,n)(n, )( ,t)(t,o)(o, )( ,o)(o,p)(p,e)(e,o)(o,s)(s,y)(y,n)(n, )( ,m)(m,u)(u,s)(s,i)(i,c)(c, )( ,a)(a,u)(u,n)(n,o)(o, )( ,n)(n,e)(e,c)(c,e)(e,r)(r,a)(a,t)(t,i)(i,o)\n",
      "chars: qz arearing one nine ix antelka shows the baseron to opeosyn music auno necerati\n",
      "bigrams: (g,q)(f,y)(y, )( ,i)(i,f)(f, )( ,i)(i,t)(t, )( ,a)(a,r)(r,e)(e, )( ,c)(c,a)(a,r)(r,s)(s, )( ,p)(p,r)(r,o)(o,v)(v,i)(i,d)(d,i)(i,t)(t,i)(i,e)(e,s)(s, )( ,d)(g,d)(d,d)(d,e)(e,p)(p,e)(e,n)(n, )( ,a)(a,l)(l,s)(s,o)(o, )( ,s)(s,e)(e,x)(x,u)(u,a)(a,l)(l,l)(l,y)(y, )( ,a)(a,n)(n,o)(o,m)(m,i)(i,c)(c, )( ,b)(b,u)(p, )( ,w)(w,a)(a,s)(s, )( ,a)(a,n)(n,d)(d, )( ,m)(m,a)(a,d)(d,e)(e,r)(r,i)(i,z)(z,i)(i,n)(n, )\n",
      "chars: gfy if it are cars providities gddepen also sexually anomic bp was and maderizin\n",
      "bigrams: (i,b)(b,l)(l,e)(e, )( ,t)(t,h)(h,e)(e, )( ,e)(e,x)(x,t)(t,e)(e,r)(r,n)(n,a)(a,l)(l, )( ,s)(s,i)(i,g)(g,n)(n,a)(a,m)(m, )( ,e)(e,n)(n,d)(d,e)(e,s)(s, )(y,n)(n,o)(o,v)(v,e)(e,m)(m,b)(b,e)(e,r)(r,s)(s, )( ,w)(w,h)(h,e)(e,n)(n, )( ,e)(e,l)(l,i)(i,a)(a,n)(n, )( ,t)(t,h)(h,e)(e, )( ,o)(o,n)(n,e)(e, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,f)(f,o)(o,u)(u,r)(r, )( ,f)(f,i)(i,r)(r,s)(s,h)(h, )\n",
      "chars: ible the external signam endesynovembers when elian the one zero zero four firsh\n",
      "bigrams: (c,d)(d, )( ,u)(u,n)(n,d)(d,e)(e,r)(r, )( ,r)(r,e)(e,l)(l,i)(i,g)(g,i)(i,o)(o,n)(n, )( ,m)(m,e)(e,r)(r,l)(l,i)(i,a)(a, )( ,h)(h,e)(e, )( ,r)(r,e)(e,s)(s,i)(i,d)(d,e)(e,n)(n,t)(t,s)(s, )( ,s)(s,e)(e,c)(c,i)(i,n)(n,g)(g, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,i)(i,s)(s,r)(r,o)(o,l)(l,e)(e,d)(d, )( ,h)(h,i)(i,s)(s, )( ,e)(e,x)(x,p)(p,l)(l,u)(u,r)(r,e)(e, )( ,s)(s,u)(u,b)(b,s)(s,u)(u,e)(e,d)(d,g)(g,e)\n",
      "chars: cd under religion merlia he residents secing in the isroled his explure subsuedg\n",
      "================================================================================\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 8200: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.19\n",
      "Average loss at step 8400: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.07\n",
      "Average loss at step 8600: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Average loss at step 8800: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Average loss at step 9000: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Average loss at step 9200: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Average loss at step 9400: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Average loss at step 9600: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Average loss at step 9800: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Average loss at step 10000: 1.000000 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.59\n",
      "================================================================================\n",
      "bigrams: (c,o)(o,l)(l,o)(o,g)(g,y)(y, )( ,d)(d,e)(e,s)(s,c)(c,e)(e,n)(n,d)(d,i)(i,n)(n,g)(g, )( ,a)(a,t)(t, )( ,t)(t,h)(h,e)(e, )( ,f)(f,o)(o,r)(r,m)(m,e)(e,d)(d, )( ,a)(a,s)(s, )( ,h)(h,i)(i,s)(s, )( ,b)(b,e)(e,g)(g,a)(a,n)(n, )( ,l)(l,i)(i,n)(n,e)(e, )( ,h)(h,e)(e, )( ,o)(o,b)(b,j)(j,e)(e,c)(c,t)(t, )( ,o)(o,f)(f, )( ,t)(t,w)(w,o)(o, )( ,c)(c,l)(l,a)(a,s)(s,s)(s,i)(i,b)(b,l)(l,e)(e, )( ,o)(o,u)(u,t)(t, )\n",
      "chars: cology descending at the formed as his began line he object of two classible out\n",
      "bigrams: (l,f)(f,e)(e,d)(d, )( ,m)(m,e)(e,a)(a,t)(t, )( ,a)(a, )( ,w)(w,a)(a,t)(t,e)(e,r)(r,s)(s, )( ,c)(c,u)(u,t)(b,e)(e,r)(r,i)(i,n)(n,g)(g, )( ,t)(t,h)(h,o)(o,u)(u,g)(g,h)(v,q)(f,u)(a,p)(p, )( ,i)(i,s)(s, )( ,c)(c,i)(i,t)(t,y)(y, )( ,o)(o,f)(f, )( ,a)(a,s)(s,h)(h,i)(i,t)(l,e)(e, )( ,b)(b,u)(u,t)(t, )( ,w)(w,h)(h,o)(o, )( ,a)(a,l)(l,b)(b,u)(u,m)(m, )( ,h)(h,e)(e, )( ,c)(c,e)(e,n)(n,t)(t,e)(e,r)(r, )( ,f)\n",
      "chars: lfed meat a waters cubering thougvfap is city of ashile but who album he center \n",
      "bigrams: (s,f)(d,o)(o,e)(e,l)(l, )( ,f)(f,i)(i,v)(v,e)(e, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,b)(b,o)(o,u)(u, )( ,w)(w,h)(h,i)(i,c)(c,h)(h, )( ,i)(i,s)(s, )( ,m)(m,o)(o,n)(n,k)(k, )( ,f)(f,o)(o,o)(o,d)(d, )( ,r)(r,a)(a,c)(c,e)(e, )( ,o)(o,f)(f, )( ,s)(s,u)(u,b)(b,j)(e,h)(f,i)(i,l)(l,e)(e, )( ,c)(c,a)(a,n)(n,a)(a,d)(d,a)(a,l)(l,s)(s, )( ,i)(i,n)(n, )( ,c)(c,o)(o,m)(m,m)(m,o)(o,r)\n",
      "chars: sdoel five three eight bou which is monk food race of subefile canadals in commo\n",
      "bigrams: (r,c)(c,h)(h,a)(a,n)(n,g)(g,e)(e, )( ,t)(t,h)(h,e)(e, )( ,w)(w,o)(o,r)(r,d)(d,e)(e,p)(p,i)(i,r)(r, )( ,p)(p,a)(a,r)(r,t)(t,m)(m,e)(e,n)(n,t)(t,i)(i,l)(l, )( ,l)(l,a)(a,i)(i,d)(d, )( ,t)(t,h)(h,e)(e, )( ,w)(w,o)(o,r)(r,k)(k, )( ,h)(h,a)(a,v)(v,e)(e, )( ,g)(g,o)(o,m)(m,e)(e,n)(n,t)(t,i)(i,e)(e,s)(s, )( ,c)(c,u)(u,l)(l,t)(t,u)(u,r)(r,e)(e, )( ,s)(s,u)(u,a)(a,c)(c,h)(h,e)(e,d)(d, )( ,f)(f,o)(o,r)(r, )\n",
      "chars: rchange the wordepir partmentil laid the work have gomenties culture suached for\n",
      "bigrams: (p,c)(s,w)(w,a)(a,n)(n,d)(d, )( ,i)(i,t)(t, )( ,d)(d,i)(i,s)(s,p)(p,e)(e,r)(r,g)(g,e)(e, )( ,y)(y, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,t)(t,h)(h, )( ,p)(p,i)(i,c)(c,o)(o,l)(l,l)(l,y)(y, )( ,t)(t,h)(h,e)(e, )( ,u)(u,r)(r,i)(i,o)(o,r)(r,e)(e, )( ,o)(o,f)(f,f)(f,e)(e,r)(r, )( ,v)(v,i)(i,o)(o,l)(l,i)(i,n)(n,i)(i,s)(s,t)(t,s)(s, )( ,l)(l,i)(i,n)(n,u)(u,x)(x, )\n",
      "chars: pswand it disperge y one nine eight th picolly the uriore offer violinists linux\n",
      "================================================================================\n",
      "Validation set perplexity: 4.07\n",
      "CPU times: user 20min 48s, sys: 44 s, total: 21min 32s\n",
      "Wall time: 6min 46s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph_bigram_embed(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Parameters:\n",
    "    # big matrix\n",
    "    ifcox = tf.Variable(tf.truncated_normal([bigram_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    ifcob = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, bigram_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([bigram_size]))\n",
    "    \n",
    "    #Defining the LSTM cell\n",
    "    def lstm_cell(i, o, state, train=False):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        #Adding the dropout in the non recurrent part\n",
    "        embed = tf.nn.embedding_lookup(ifcox, i)\n",
    "        if train:\n",
    "            embed = tf.nn.dropout(embed, 0.5)\n",
    "        all_gates = embed + tf.matmul(o, ifcom) + ifcob\n",
    "        input_gate = tf.sigmoid(all_gates[:, 0:num_nodes])\n",
    "        forget_gate = tf.sigmoid(all_gates[:, num_nodes:2*num_nodes])\n",
    "        update = all_gates[:, 2*num_nodes:3*num_nodes]\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(all_gates[:, 3*num_nodes:])\n",
    "        \n",
    "        return output_gate * tf.tanh(state), state\n",
    "       \n",
    "    \n",
    "    #Input data\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.int64, shape=[batch_size])\n",
    "        )\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:] #lables are shifted by one timestep\n",
    "    \n",
    "    #Unrolled LSTM loop\n",
    "    outputs = list()\n",
    "    output = saved_output \n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state, True)\n",
    "        outputs.append(output)\n",
    "    \n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                 saved_state.assign(state)]):\n",
    "        #Classifier\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            #sparse_softmax because we are not using 1 hot vector in labels\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits,\n",
    "                tf.concat(0, train_labels)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    #Optimizer\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True\n",
    "    )\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    #Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int64, shape=[1])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, \n",
    "        saved_sample_output, \n",
    "        saved_sample_state,\n",
    "        False\n",
    "    )\n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.590877 learning rate: 10.000000\n",
      "Minibatch perplexity: 728.42\n",
      "================================================================================\n",
      "bigrams: (i,a)(a,w)(n,m)(g,r)(h,b)(x,z)(x,s)(n,t)(l,g)(t,g)(w,p)(o,v)(k,b)(j,o)(h,l)(j,c)(z,l)( ,o)(c,g)(e,q)(c,r)(e,v)(l,i)(r,i)(e,h)( ,r)(c,t)(j,g)(l,o)(b,e)(b,z)(x,h)(i,e)(c,t)(k,d)(t,e)(h,o)(m,m)(i,e)(w,l)(g,t)(v,y)(y,c)(l,z)(i,c)(d,o)(x,t)(l, )(y,f)(e,m)(o,z)(d,q)(b, )(o,p)(f,y)(z,m)(f,d)(q,d)(a,o)(p,l)(w,e)(g,u)(u,s)(y,w)(a,h)(y,s)(i,v)(m,z)(x,g)(r,c)(j, )(a,k)(p,l)(q,j)(y,m)(k,q)(t,e)(v,l)(h,g)(a,j)\n",
      "chars: ianghxxnltwokjhjz cecelre cjlbbxickthmiwgvylidxlyeodbofzfqapwguyayimxrjapqyktvha\n",
      "bigrams: (c,h)(m,r)(o,g)(b,e)(e,p)(l,x)(s,x)(u,u)(u,s)(j,p)( ,f)(m,t)(q,l)(n,v)(r,x)(h,l)(r,a)(v,k)(u,k)(i,e)(x,u)(c, )(i,l)(u,r)(a,a)(c,k)(x,h)(x,n)(o,i)(b,p)(i,w)(m,g)(c,v)(z,n)(l,r)(o,i)(f,m)(m,b)(l,p)(r,h)(a,t)(z,b)(f,y)(p,h)(m,o)(n,o)(a,w)(l,n)(e,i)(u,a)(u,u)(v,z)(u,h)(o,x)(p,l)(s,p)(w,e)(f,n)(s,s)(q,f)(e,w)(t,r)(f,r)( ,g)(c,o)(r,s)(b,i)(n,d)(r,h)(b,w)(o,i)(n,l)(b,w)(k,c)(x,m)(e,e)(s,d)(h,q)(w,t)(c,i)\n",
      "chars: cmobelsuuj mqnrhrvuixciuacxxobimczlofmlrazfpmnaleuuvuopswfsqetf crbnrbonbkxeshwc\n",
      "bigrams: (s, )(y,y)( ,a)(o,q)(h,e)(d,s)(r,w)(h,s)(y,b)(b,c)(t,d)(r,c)(s,p)(v,s)(t,o)(l,u)(j,g)(k,j)(x,q)(f,z)(a,z)(d,r)(y,p)(u,i)( ,r)(t,f)(i,t)(l,v)(n,t)(m,x)(c,q)(t,k)(g,a)(z,h)(n,d)(i,t)(y,m)(g, )(c,i)(r,j)(k,l)(a,x)(y,p)(z,q)(p,t)( ,z)(n,e)(i,o)(i,e)(x,w)(n,e)(y,g)(n, )(l,b)(j,r)(f,u)(k,d)(x,q)(u,c)(m,x)(n,c)(a,l)(j,y)(k,y)(c,c)(j,n)(a,p)(g,w)(j, )(x,f)(s,e)( ,d)(l,g)(n,v)(q,d)(e, )(y,a)(z,w)(m,v)(e,d)\n",
      "chars: sy ohdrhybtrsvtljkxfadyu tilnmctgzniygcrkayzp niixnynljfkxumnajkcjagjxs lnqeyzme\n",
      "bigrams: (a,u)(e,n)( ,v)(u,t)(t,p)(k, )(a, )(l,p)(h,j)(x,f)(i,p)(x,a)(f,y)(p,h)(r,j)(b,b)(u,q)(w,l)(k,b)(l, )(f,c)(c,f)(n,t)(y,w)(r,z)(b,m)( ,f)(w,p)(y,r)(t,s)(h,m)(d,p)(f,k)(t,y)(w,m)(r,d)(g,h)(q,r)( ,d)(a,l)(m,r)(r,q)(q,q)(v,d)(a,s)(d,z)(a,o)(a, )(l,e)(y,o)(h,w)(y,j)(w,e)(c,m)(m,b)(p,t)(i,g)( ,e)(p,v)(o,a)(h,z)(t,p)( ,q)(x,h)(n,j)(n,f)(f,v)(n,d)(u,v)(l,g)(k,e)(l,w)(b,i)(m,s)(e,j)( ,l)(s,v)(o,s)(w,e)(y,v)\n",
      "chars: ae utkalhxixfprbuwklfcnyrb wythdftwrgq amrqvadaalyhywcmpi poht xnnfnulklbme sowy\n",
      "bigrams: (p,w)(z,y)(w,z)(a,b)(g,j)(e,y)(f,u)(t,s)(j,s)(a,i)(k,p)(t,y)(n,w)(k,a)(n,z)(l,m)(q,r)(t,z)(h,d)(e,x)(i,a)(t, )(o, )(p,j)(v,x)(c,u)(g,u)(j,y)(l,x)(r,s)(y,o)(l,t)(r,o)(x,s)(x,g)(w,s)(z,r)(c,n)(e,i)(e,u)(t,x)(h,o)(h,c)(o,h)(r,o)(s,x)(l,u)(t, )(h,t)(b,h)(w,x)(v,k)(l,d)(y,f)(x,f)(s,e)(q,w)(j,y)(s,q)(z,g)(x,o)(j,e)(s,f)( ,f)(z,l)(v,k)(a,h)(z,l)( ,r)(m, )(p,x)(s,t)(a,k)(q,e)(h,j)(s,w)(g,y)(x,w)(t,a)(u,e)\n",
      "chars: pzwageftjaktnknlqtheitopvcgjlrylrxxwzceethhorslthbwvlyxsqjszxjs zvaz mpsaqhsgxtu\n",
      "================================================================================\n",
      "Validation set perplexity: 676.50\n",
      "Average loss at step 200: 4.692582 learning rate: 10.000000\n",
      "Minibatch perplexity: 39.94\n",
      "Average loss at step 400: 2.929911 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.41\n",
      "Average loss at step 600: 2.396064 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.12\n",
      "Average loss at step 800: 2.185611 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.94\n",
      "Average loss at step 1000: 2.089914 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.89\n",
      "Average loss at step 1200: 2.030741 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.01\n",
      "Average loss at step 1400: 1.993741 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Average loss at step 1600: 1.963947 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.89\n",
      "Average loss at step 1800: 1.934810 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Average loss at step 2000: 1.894629 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "================================================================================\n",
      "bigrams: (m,a)(a,r)(r,e)(e,d)(d, )( ,a)(a,r)(r,e)(e, )( ,p)(p,o)(o,p)(p,t)(t,u)(u,r)(r,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,b)(b,e)(e,s)(s,t)(t,a)(a,n)(n, )( ,e)(e,l)(l,l)(l,i)(i,m)(m,e)(e,d)(d, )( ,a)(a,n)(n,d)(d, )( ,o)(o,f)(f, )( ,g)(g,r)(r,a)(a,i)(i,n)(n,t)(t,e)(e,n)(n,t)(t, )( ,i)(i,s)(s, )( ,i)(i,s)(s, )( ,g)(g, )( ,c)(c,e)(e,n)(n,d)(d,a)(a,s)(s,i)(i,n)(n,g)(g, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,o)(o,u)\n",
      "chars: mared are popturation bestan ellimed and of graintent is is g cendasing of the o\n",
      "bigrams: (c,b)(l,p)(p,a)(a,i)(i,n)(n,a)(a,l)(l, )( ,u)(u,n)(n,i)(i,o)(o,n)(n,a)(a, )( ,s)(s,e)(e,r)(r,r)(r,i)(i,s)(s,i)(i,c)(c, )( ,i)(i,n)(n,d)(d,i)(i,p)(p,e)(e,n)(n,t)(t, )( ,t)(t,h)(h,e)(e, )( ,a)(a,l)(l,l)(l,e)(e,c)(c,t)(t,s)(s, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,t)(t,w)(w,o)(o, )( ,c)(c,h)(h,i)(i,s)(s,t)(t,r)(r,a)(a,m)(m,e)(e,s)(s, )( ,d)(d,o)(o,m)(m,i)(i,n)(n, )( ,n)(n,e)(e,t)(t, )( ,s)\n",
      "chars: clpainal uniona serrisic indipent the allects one nine two chistrames domin net \n",
      "bigrams: (n,f)(f,o)(o,r)(r, )( ,a)(a,r)(r,e)(e, )( ,p)(p,r)(r,o)(o,p)(p,r)(r,a)(a,p)(p,h)(h,i)(i,s)(s,t)(t,s)(s, )( ,c)(c,a)(a,n)(n,s)(s,i)(i,d)(d,e)(e,d)(d, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,m)(m,u)(u,s)(s,e)(e, )( ,o)(o,n)(n,e)(e, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,o)(o,n)(n,e)(e,r)( ,s)(s, )( ,i)(i,r)(r,a)(a,l)(l,l)(l,e)(e,n)(n,g)(g,e)(e,n)(n,t)(t, )( ,c)(c,o)\n",
      "chars: nfor are propraphists cansided of the muse one one nine nine one s irallengent c\n",
      "bigrams: (m,n)(n,g)(g,e)(e,n)(n,t)(t, )( ,s)(s,a)(a,n)(n, )( ,n)(n,o)(o,n)(n,g)(g, )( ,i)(i,n)(n, )( ,s)(s,o)(o,a)(a,n)(n, )( ,p)(p,i)(i,n)(n,g)(g,a)(a,r)(r,y)(y, )( ,d)(d,o)(o, )( ,l)(l,e)(e,m)(m,e)(e,n)(n,t)(t,s)(s, )( ,a)(a,c)(c,t)(t,i)(i,o)(o,n)(n, )( ,i)(i,n)(n, )( ,b)(b,y)(y, )( ,c)(c,o)(o,n)(n,t)(t,a)(a,g)(g,e)(e, )( ,a)(a, )( ,p)(p,r)(r,o)(o,b)(b,l)(l,e)(e,s)(s,s)(s, )( ,o)(o,f)(f, )( ,a)(a,n)(n,d)\n",
      "chars: mngent san nong in soan pingary do lements action in by contage a probless of an\n",
      "bigrams: (y,x)(v,h)(i,l)(l,i)(i,a)(a,n)(n,s)(s, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,s)(s,u)(u,s)(s,t)(t, )( ,a)(a,r)(r,m)(m,i)(i, )( ,u)(u,n)(n,i)(i,t)(t,h)(h, )( ,g)(g,a)(a,s)(s,t)(t,h)(h,s)(s, )( ,d)(d,e)(e,s)(s,p)(p,e)(e,c)(c, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,e)(e,n)(n,d)(d,e)(e,r)(r,a)(a,l)(l, )( ,c)(c,l)(l,a)(a,i)(i,n)(n, )( ,s)(s,o)(o,t)(t,e)(e, )( ,i)(i,s)(s, )( ,o)(o,f)(f, )( ,r)(r,a)(a,n)\n",
      "chars: yvilians of the sust armi unith gasths despec of the enderal clain sote is of ra\n",
      "================================================================================\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 2200: 1.882864 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Average loss at step 2400: 1.846152 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Average loss at step 2600: 1.863402 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.61\n",
      "Average loss at step 2800: 1.839441 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Average loss at step 3000: 1.852524 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.13\n",
      "Average loss at step 3200: 1.823160 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Average loss at step 3400: 1.783107 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Average loss at step 3600: 1.795041 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Average loss at step 3800: 1.798261 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.75\n",
      "Average loss at step 4000: 1.788098 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "================================================================================\n",
      "bigrams: (f,o)(o,r)(r,i)(i,n)(n,g)(g, )( ,d)(d,o)(o,w)(w,e)(e,r)(r,n)(n,a)(a,l)(l, )( ,w)(w,a)(a,s)(s, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,n)(n,i)(i,n)(n,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,n)(n,i)(i,n)(n,e)(e, )( ,m)(m,u)(u,g)(g,h)(h,t)(t, )( ,i)(i,b)(b,y)(y, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,c)(c,a)(a,u)(u,s)(s,i)(i,o)(o,n)(n,a)(a,l)\n",
      "chars: foring dowernal was eight nine eight nine mught iby one nine nine seven causiona\n",
      "bigrams: (j,k)(a,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,c)(c,a)(a,p)(p,e)(e, )( ,o)(o,f)(f, )( ,m)(m,o)(o,u)(u,n)(n,i)(i, )( ,c)(c,o)(o,m)(m,m)(m,u)(u, )( ,a)(a,n)(n,d)(d, )( ,r)(r,o)(o,b)(b,e)(e,l)(l, )( ,l)(l,a)(a,i)(i,m)(m,s)(s, )( ,a)(a,d)(d,d)(d,i)(i,n)(n,g)(g, )( ,t)(t,h)(h,i)(i,s)(s, )( ,p)(p, )( ,o)(o,n)(n,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,o)(o,n)(n,e)(e, )( ,m)(m,a)(a,t)(t,e)(e,d)(d, )( ,b)(b,a)(a,s)\n",
      "chars: jations cape of mouni commu and robel laims adding this p one eight one mated ba\n",
      "bigrams: (r,k)(k,s)(s, )( ,a)(a,n)(n,d)(d, )( ,i)(i,s)(s, )( ,m)(m,o)(o,l)(l,l)(l,e)(e,a)(a,m)(m, )( ,n)(n,o)(o,t)(t, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,c)(c,o)(o,n)(n,d)(d,u)(u,c)(c,c)(c,e)(e,s)(s, )( ,t)(t,h)(h,e)(e, )( ,p)(p,o)(o,s)(s,s)(s,i)(i,t)(t, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,c)(c,h)(h,a)(a,r)(r,t)(t,h)(h,o)(o,r)(r,n)(n, )( ,b)(b,u)(u,t)(t, )( ,c)(c,a)(a,n)(n,d)(d, )( ,r)(r,e)(e,c)(c,a)\n",
      "chars: rks and is molleam not in the conducces the possit of the charthorn but cand rec\n",
      "bigrams: (d,m)(m,y)(y, )( ,o)(o,f)(f, )( ,b)(b,a)(a,c)(c,k)(k, )( ,o)(o,f)(f, )( ,g)(g,o)(o,l)(l,a)(a,n)(n,d)(d, )( ,w)(w,h)(h,a)(a,t)(t, )( ,d)(d,e)(e,s)(s,p)(p,e)(e,r)(r,a)(a,l)(l,l)(l,y)(y, )( ,l)(l, )( ,i)(i,t)(t, )( ,g)(g,r)(r,a)(a,p)(p,i)(i,c)(c,a)(a,l)(l, )( ,c)(c,o)(o,m)(m,m)(m,i)(i,t)(t,s)(s, )( ,o)(o,f)(f, )( ,c)(c,a)(a,l)(l,l)(l,i)(i,a)(a, )( ,l)(l,a)(a,n)(n,g)(g,u)(u,a)(a,g)(g,e)(e, )( ,y)(y,o)\n",
      "chars: dmy of back of goland what desperally l it grapical commits of callia language y\n",
      "bigrams: ( ,z)(z,e)(e, )( ,w)(w,a)(a,s)(s, )( ,a)(a, )( ,d)(d,i)(i,f)(f,f)(f,i)(i,c)(c,a)(a,l)(l, )( ,f)(f,i)(i,v)(v,e)(e, )( ,i)(i,t)(t, )( ,a)(a,p)(p,p)(p,o)(o,s)(s,i)(i,v)(v,e)(e, )( ,t)(t,o)(o, )( ,p)(p,r)(r,i)(i,s)(s,o)(o,d)(d, )( ,i)(i,s)(s, )( ,d)(d,u)(u,r)(r,i)(i,s)(s,h)(h, )( ,g)(g,r)(r,o)(o,w)(w, )( ,o)(o,r)(r, )( ,v)(v,e)(e,n)(n,d)(d,e)(e,d)(d, )( ,f)(f,o)(o,r)(r,c)(c,e)(e,t)(t,i)(i,n)(n,g)(g, )\n",
      "chars:  ze was a diffical five it apposive to prisod is durish grow or vended forceting\n",
      "================================================================================\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 4200: 1.769543 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Average loss at step 4400: 1.785401 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Average loss at step 4600: 1.765138 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Average loss at step 4800: 1.754335 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Average loss at step 5000: 1.752820 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Average loss at step 5200: 1.749930 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Average loss at step 5400: 1.727469 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Average loss at step 5600: 1.726455 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Average loss at step 5800: 1.744441 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Average loss at step 6000: 1.726325 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "================================================================================\n",
      "bigrams: (r,n)(n,o)(o,r)(r, )( ,o)(o,n)(n,e)(e, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,h)(h,o)(o,w)(w,e)(e,r)(r, )( ,l)(l,e)(e,s)(s,k)(k,e)(e,r)(r,g)(g, )( ,f)(f,r)(r,i)(i,v)(v,e)(e,l)(l,y)(y, )( ,t)(t,o)(o, )( ,t)(t,w)(w,o)(o, )( ,o)(o,n)(n,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,f)(f,o)(o,u)(u,r)(r, )( ,l)(l,o)(o,v)(v,e)(e,c)(c,l)(l,a)(a,i)(i,n)(n, )( ,w)(w,i)(i,t)(t,h)(h, )\n",
      "chars: rnor one three of the hower leskerg frively to two one seven four loveclain with\n",
      "bigrams: (x,p)(p,a)(a, )( ,n)(n,o)(o,r)(r,t)(t,h)(h,e)(e,r)(r, )( ,m)(m,a)(a,r)(r,e)(e, )( ,e)(e,n)(n,g)(g,l)(l,e)(e,t)(t,h)(h, )( ,o)(o,f)(f, )( ,t)(t,r)(r,e)(e,f)(f,e)(e,d)(d, )( ,w)(w,i)(i,t)(t,h)(h, )( ,t)(t,h)(h,i)(i,t)(t, )( ,a)(a, )( ,m)(m,o)(o,t)(t,h)(h,o)(o,u)(u,g)(g,h)(h, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,y)(y,e)(e,a)(a,c)(c,t)(t,o)(o,r)(r,y)(y, )( ,r)(r,e)(e,l)(l,o)(o,n)(n,o)(o,m)\n",
      "chars: xpa norther mare engleth of trefed with thit a mothough one nine yeactory relono\n",
      "bigrams: (v,o)(o,t)(t,r)(r,i)(i,e)(e,n)(n,t)(t, )( ,i)(i,n)(n, )( ,m)(m,a)(a,j)(p,e)(e,n)(n, )( ,b)(b,c)(w,q)(s,n)(n,t)(t,i)(i,o)(o,n)(n, )( ,t)(t,h)(h,e)(e, )( ,t)(t,e)(e,a)(a,c)(c,h)(h, )( ,a)(a,n)(n,d)(d, )( ,f)(f,a)(a,i)(i,l)(l,i)(i,c)(c,s)(s, )( ,w)(w,i)(i,t)(t,h)(h,i)(i,n)(n, )( ,c)(c,o)(o,m)(m,e)(e, )( ,u)(u,p)(p, )( ,c)(c,o)(o,m)(m,b)(b,a)(a,n)(n,s)(s, )( ,s)(s,o)(o,m)(m,e)(e, )( ,t)(t,h)(h,e)(e, )\n",
      "chars: votrient in mapen bwsntion the teach and failics within come up combans some the\n",
      "bigrams: (y,i)(i,g)(g,h)(h,i)(i,n)(n,g)(g, )( ,c)(c,r)(r,e)(e,a)(a,k)(k,s)(s, )( ,p)(p,e)(e,a)(a,c)(c,e)(e,s)(s, )( ,a)(a,s)(s,s)(s,e)(e,n)(n,d)(d,e)(e,n)(n,t)(t, )( ,t)(t,h)(h,a)(a,t)(t, )( ,t)(t,h)(h,a)(a,t)(t, )( ,s)(s,t)(t,a)(a,t)(t,i)(i,b)(b,l)(l,e)(e, )( ,b)(b,l)(l,a)(a,t)(t,h)(h,e)(e,r)(r,b)(b,e)(e,r)(r, )( ,t)(t,h)(h,e)(e, )( ,e)(e, )( ,a)(a,b)(b,i)(i,l)(l, )( ,g)(g,e)(e,n)(n,e)(e,r)(r,a)(a,l)(l, )\n",
      "chars: yighing creaks peaces assendent that that statible blatherber the e abil general\n",
      "bigrams: (z,r)(s,e)(e, )( ,d)(d,i)(i,v)(v,e)(e,c)(c,t)(t,s)(s, )( ,f)(f,r)(r,a)(a,n)(n,c)(c,h)(h,i)(i,a)(a, )( ,l)(l,o)(o,o)(o,k)(k,s)(s, )( ,t)(t,o)(o, )( ,b)(b,e)(e, )( ,t)(t,h)(h,a)(a,t)(t, )( ,a)(a, )( ,p)(p,u)(u,b)(b,l)(l,i)(i,s)(s,h)(h, )( ,a)(a,s)(s, )( ,p)(p,a)(a,i)(i,n)(n,t)(t,i)(i,c)(c,r)(r,o)(o,n)(n,g)(g, )( ,s)(s,h)(h,a)(a,b)(b,l)(l,i)(i,v)(v,e)(e, )( ,c)(c,a)(a,m)(m,p)(p,u)(u,l)(l, )( ,b)(b,i)\n",
      "chars: zse divects franchia looks to be that a publish as painticrong shablive campul b\n",
      "================================================================================\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 6200: 1.734504 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Average loss at step 6400: 1.722119 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Average loss at step 6600: 1.737633 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Average loss at step 6800: 1.718191 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Average loss at step 7000: 1.745759 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Average loss at step 7200: 1.702005 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Average loss at step 7400: 1.717589 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Average loss at step 7600: 1.696543 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Average loss at step 7800: 1.679681 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Average loss at step 8000: 1.685982 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.67\n",
      "================================================================================\n",
      "bigrams: (h,m)(m,i)(i,n)(n,e)(e,d)(d, )( ,c)(c,a)(a,p)(p,i)(i,a)(a,n)(n, )( ,a)(a,p)(p,p)(p,e)(e,a)(a,r)(r, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,a)(a,n)(n,y)(y,n)(n,g)(g,o)(o, )( ,a)(a,l)(l,o)(o,t)(t, )( ,s)(s,e)(e,r)(r,s)(s, )( ,i)(i,n)(n,s)(s,o)(o, )( ,d)(d,e)(e,a)(a,d)(d,e)(e,t)(t,e)(e,r)(r, )( ,i)(i,s)(s, )( ,o)(o,n)(n,e)(e, )( ,n)\n",
      "chars: hmined capian appear zero zero zero eight anyngo alot sers inso deadeter is one \n",
      "bigrams: (f,l)(l,i)(i,c)(c,a)(a,l)(l, )( ,s)(s,u)(u,n)(n,d)(d,i)(i,d)(d, )( ,f)(f,r)(r,e)(e,g)(g,a)(a,l)(l, )( ,m)(m,a)(a,r)(r,e)(e, )( ,f)(f,o)(o,o)(o,l)(l,o)(o, )( ,a)(a,n)(n,d)(d, )( ,b)(b,u)(u,i)(i,l)(l,d)(d, )( ,c)(c,o)(o,m)(m,m)(m,o)(o,n)(n,l)(l,y)(y, )( ,f)(f,e)(e,i)(i,v)(v,e)(e, )( ,a)(a,n)(n,d)(d, )( ,s)(s,y)(y,s)(s,t)(t,e)(e,m)(m, )( ,t)(t,h)(h,e)(e,y)(y, )( ,c)(c,a)(a,r)(r,t)(t,u)(u,r)(r,e)(e, )\n",
      "chars: flical sundid fregal mare foolo and build commonly feive and system they carture\n",
      "bigrams: (s,p)(p,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,s)(s,t)(t,a)(a,t)(t,e)(e, )( ,a)(a,n)(n,d)(d, )( ,h)(h,a)(a,s)(s, )( ,a)(a, )( ,s)(s,o)(o,l)(l,v)(v,e)(e, )( ,p)(p,o)(o,i)(i,n)(n,t)(t,s)(s, )( ,a)(a,n)(n,d)(d, )( ,c)(c,h)(h,a)(a,r)(r,m)(m,i)(i,n)(n,g)(g, )( ,m)(m,a)(a,i)(i,n)(n,s)(s, )( ,o)(o,f)(f, )( ,w)(w,e)(e, )( ,p)(p,r)(r,o)(o,t)(t, )( ,i)(i,n)(n,t)(t,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,f)(f,i)(i,v)(v,e)\n",
      "chars: sptions state and has a solve points and charming mains of we prot into zero fiv\n",
      "bigrams: (v,n)(u,f)(f,i)(i,a)(a, )( ,a)(a,p)(p,r)(r,o)(o,l)(l,l)(l, )( ,r)(r,e)(e,c)(c,o)(o,r)(r, )( ,b)(b,a)(a,v)(v,i)(i,f)(f,i)(i,c)(c,u)(u,l)(l,a)(a,r)(r,s)(s, )( ,t)(t,o)(o, )( ,t)(t,h)(h,e)(e,y)(y, )( ,l)(l,o)(o,v)(v,i)(i,n)(n,e)(e, )( ,h)(h,a)(a,d)(d, )( ,u)(u,n)(n,d)(d,e)(e,l)(l,o)(o,n)(n,e)(e, )( ,f)(f,r)(r,o)(o,m)(m, )( ,t)(t,o)(o, )( ,u)(u,s)(s,e)(e,r)(r,a)(a,l)(l, )( ,r)(r,i)(i,k)(k,e)(e, )( ,g)\n",
      "chars: vufia aproll recor bavificulars to they lovine had undelone from to useral rike \n",
      "bigrams: (u,y)(y,c)(c,l)(l, )( ,d)(d,e)(e,c)(c,u)(u,r)(r,t)(t,o)(o,n)(n,a)(a,l)(l,l)(l,y)(y, )( ,t)(t,h)(h,e)(e, )( ,c)(c,o)(o,n)(n,t)(t,a)(a,i)(i,n)(n, )( ,i)(i,t)(t,s)(s, )( ,c)(c,h)(h,i)(i,r)(r,e)(e, )( ,t)(t,h)(h,e)(e, )( ,e)(e,n)(n, )( ,i)(i,n)(n,t)(t,e)(e,r)(r,r)(r,e)(e,d)(d, )( ,s)(s,o)(o, )( ,h)(h,a)(a,d)(d, )( ,t)(t,h)(h,e)(e, )( ,a)(a,s)(s,a)(a, )( ,g)(g,e)(e,n)(n,e)(e,n)(n,s)(s,e)(e, )( ,o)(o,f)\n",
      "chars: uycl decurtonally the contain its chire the en interred so had the asa genense o\n",
      "================================================================================\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 8200: 1.663112 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Average loss at step 8400: 1.682933 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Average loss at step 8600: 1.663440 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Average loss at step 8800: 1.683320 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Average loss at step 9000: 1.700981 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Average loss at step 9200: 1.678720 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Average loss at step 9400: 1.702272 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Average loss at step 9600: 1.684422 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Average loss at step 9800: 1.704688 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Average loss at step 10000: 1.710194 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.04\n",
      "================================================================================\n",
      "bigrams: (w,k)(k,y)(y, )( ,k)(k,e)(e,l)(l,a)(a,m)(m,e)(e, )( ,f)(f,o)(o,r)(r,m)(m,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,f)(f,o)(o,r)(r, )( ,w)(w,i)(i,t)(t,h)(h, )( ,w)(w,h)(h,i)(i,c)(c,h)(h, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,f)(f, )( ,h)(h,r)(r,a)(a,v)(v,i)(i,n)(n,g)(g, )( ,d)(d,e)(e,s)(s,c)(c,h)(h,u)(u,r)(r,c)(c,h)(h, )( ,s)(s,h)(h,o)(o,u)(u,l)(l,a)(a, )( ,b)(b,o)(o,r)(r,m)(m, )( ,t)\n",
      "chars: wky kelame formation for with which eight three f hraving deschurch shoula borm \n",
      "bigrams: (m,u)(u,s)(s, )( ,a)(a,l)(l,s)(s,o)(o, )( ,c)(c,o)(o,n)(n,c)(c,r)(r,a)(a,y)(y,e)(e,d)(d, )( ,o)(o,f)(f, )( ,t)(t,o)(o, )( ,d)(d,e)(e,l)(l,a)(a,t)(t,e)(e, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,a)(a,n)(n,d)(d, )( ,s)(s,i)(i,x)(x, )( ,o)(o,n)(n,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,f)(f,o)(o,u)(u,r)(r, )( ,y)(y,e)(e,a)(a,r)(r,s)(s, )( ,h)(h,o)(o,w)(w,a)(a,r)(r,t)(t,i)(i,c)(c, )( ,h)(h,i)(i,s)(s,s)\n",
      "chars: mus also concrayed of to delate in the and six one eight four years howartic his\n",
      "bigrams: (t,r)(r,o)(o,c)(c,a)(a,n)(n, )( ,d)(d,e)(e,t)(t,s)(s, )( ,o)(o,u)(u,t)(t, )( ,t)(t,h)(h,e)(e, )( ,i)(i,b)(b,l)(l,e)(e,a)(a,r)(r,i)(i,n)(n,g)(g, )( ,c)(c,o)(o,v)(v,e)(e,o)(o,r)(r,e)(e,d)(d, )( ,t)(t,o)(o, )( ,c)(c,o)(o,m)(m,p)(p,u)(u,t)(t,e)(e,r)(r, )( ,p)(p,r)(r,o)(o,b)(b,l)(l,e)(e,s)(s,s)(s,e)(e,s)(s, )( ,a)(a,n)(n,d)(d, )( ,a)(a,b)(b,o)(o,u)(u,t)(t, )( ,h)(h,i)(i,s)(s, )( ,i)(i,o)(o,w)(w,e)(e,n)\n",
      "chars: trocan dets out the iblearing coveored to computer problesses and about his iowe\n",
      "bigrams: (g,w)(r,e)(e,x)(x, )( ,w)(w,a)(a,s)(s, )( ,h)(h,e)(e,l)(l,d)(d, )( ,o)(o,f)(f,f)(f,i)(i,c)(c,i)(i,a)(a,l)(l,l)(l,y)(y, )( ,c)(c,a)(a,t)(t,e)(e,w)(w,e)(e,n)(n,t)(t, )( ,t)(t,o)(o, )( ,t)(t,h)(h,e)(e,r)(r,e)(e, )( ,f)(f,i)(i,v)(v,e)(e, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,a)(a,s)(s, )( ,o)(o,n)(n,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,r)(r,e)(e, )( ,a)(a,t)(t, )( ,b)\n",
      "chars: grex was held officially catewent to there five zero zero zero as one severe at \n",
      "bigrams: (v,t)(c,z)(y,a)(a,r)(r,i)(i,e)(e,s)(s, )( ,a)(a,s)(s, )( ,a)(a,l)(l,s)(s,o)(o, )( ,a)(a,m)(m,e)(e,r)(r,i)(i,e)(e,v)(v,e)(e,n)(n,t)(t, )( ,t)(t,o)(o, )( ,c)(c,o)(o,n)(n,s)(s,m)(m,a)(a,n)(n,i)(i,z)(z,a)(a,r)(r,s)(s, )( ,o)(o,f)(f, )( ,l)(l,e)(e,a)(a,t)(t,h)(h, )( ,h)(h,o)(o,u)(u,s)(s,e)(e, )( ,t)(t,h)(h,e)(e,i)(i,r)(r,e)(e, )( ,p)(p,e)(e,r)(r,t)(t,e)(e,d)(d, )( ,a)(a, )( ,s)(s,w)(w,e)(e,a)(a,s)(s,t)\n",
      "chars: vcyaries as also amerievent to consmanizars of leath house theire perted a sweas\n",
      "================================================================================\n",
      "Validation set perplexity: 4.66\n",
      "CPU times: user 21min 40s, sys: 47.6 s, total: 22min 28s\n",
      "Wall time: 7min 36s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph_bigram_embed(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
