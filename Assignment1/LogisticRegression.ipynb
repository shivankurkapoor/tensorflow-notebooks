{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://commondatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "data_root = '/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1' # Change me to store data elsewhere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "  \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "  slow internet connections. Reports every 5% change in download progress.\n",
    "  \"\"\"\n",
    "  global last_percent_reported\n",
    "  percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "    \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  dest_filename = os.path.join(data_root, filename)\n",
    "  if force or not os.path.exists(dest_filename):\n",
    "    print('Attempting to download:', filename) \n",
    "    filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(dest_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', dest_filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "  return dest_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download: notMNIST_large.tar.gz\n",
      "0%....5%....10%....15%....20%....25%....30%....35%....40%....45%....50%....55%....60%....65%....70%....75%....80%....85%....90%....95%....100%\n",
      "Download Complete!\n",
      "Found and verified /Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_large.tar.gz\n",
      "Attempting to download: notMNIST_small.tar.gz\n",
      "0%....5%....10%....15%....20%....25%....30%....35%....40%....45%....50%....55%....60%....65%....70%....75%....80%....85%....90%....95%....100%\n",
      "Download Complete!\n",
      "Found and verified /Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_small.tar.gz\n"
     ]
    }
   ],
   "source": [
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_large already present - Skipping extraction of /Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_large.tar.gz.\n",
      "['/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_large/A', '/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_large/B', '/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_large/C', '/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_large/D', '/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_large/E', '/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_large/F', '/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_large/G', '/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_large/H', '/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_large/I', '/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_large/J']\n",
      "/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_small already present - Skipping extraction of /Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_small.tar.gz.\n",
      "['/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_small/A', '/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_small/B', '/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_small/C', '/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_small/D', '/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_small/E', '/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_small/F', '/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_small/G', '/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_small/H', '/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_small/I', '/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_small/J']\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "    root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "    if os.path.isdir(root) and not force:\n",
    "      # You may override by setting force=True.\n",
    "      print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "    else:\n",
    "        print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "        tar = tarfile.open(filename)\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall(data_root)\n",
    "        tar.close()\n",
    "    data_folders = [\n",
    "      os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "      if os.path.isdir(os.path.join(root, d))]\n",
    "    if len(data_folders) != num_classes:\n",
    "        raise Exception(\n",
    "        'Expected %d folders, one per class. Found %d instead.' % (\n",
    "         num_classes, len(data_folders)))\n",
    "    print(data_folders)\n",
    "    return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size = 28 #Pixel width and height\n",
    "pixel_depth = 255.0 #Number of levels per pixel\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "    \"\"\"Load the data for the single letter image\"\"\"\n",
    "    image_file = os.listdir(folder)\n",
    "    dataset = np.ndarray(shape=(len(image_file), image_size, image_size), \n",
    "                        dtype=np.float32)\n",
    "    print(folder)\n",
    "    num_images = 0\n",
    "    for i, image in enumerate(image_file):\n",
    "        image_file = os.path.join(folder, image)\n",
    "        try:\n",
    "            \n",
    "            image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                         pixel_depth / 2) / pixel_depth\n",
    "            if image_data.shape != (image_size, image_size):\n",
    "                raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "            dataset[i,:,:]  = image_data\n",
    "            num_images = i+1\n",
    "        except IOError as e:\n",
    "            print('Could not read: ', image_file, ':', e, '--skipping')\n",
    "    \n",
    "    dataset = dataset[0:num_images, :,:]\n",
    "    if num_images < min_num_images:\n",
    "        raise Exception('Too few images than expected: %d < %d' %\n",
    "                       (num_images < min_num_images))\n",
    "    \n",
    "    print('Full dataset tensor:', dataset.shape)\n",
    "    print('Mean:', np.mean(dataset))\n",
    "    print('Sigma:', np.std(dataset))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "    dataset_names = []\n",
    "    for folder in data_folders:\n",
    "        set_filename = folder + '.pickle'\n",
    "        dataset_names.append(set_filename)\n",
    "        if os.path.exists(set_filename) and not force:\n",
    "            print('%s already present' % set_filename)\n",
    "        else:\n",
    "            print('Pickling %s' % set_filename)\n",
    "            dataset = load_letter(folder, min_num_images_per_class)\n",
    "            try:\n",
    "                with open(set_filename, 'wb') as f:\n",
    "                    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "            except Exception as e:\n",
    "                print('Unable to save data to', set_filename, ':', e)\n",
    "    return dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_large/A.pickle already present\n",
      "/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_large/B.pickle already present\n",
      "/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_large/C.pickle already present\n",
      "/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_large/D.pickle already present\n",
      "/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_large/E.pickle already present\n",
      "/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_large/F.pickle already present\n",
      "/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_large/G.pickle already present\n",
      "/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_large/H.pickle already present\n",
      "/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_large/I.pickle already present\n",
      "/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_large/J.pickle already present\n",
      "/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_small/A.pickle already present\n",
      "/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_small/B.pickle already present\n",
      "/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_small/C.pickle already present\n",
      "/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_small/D.pickle already present\n",
      "/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_small/E.pickle already present\n",
      "/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_small/F.pickle already present\n",
      "/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_small/G.pickle already present\n",
      "/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_small/H.pickle already present\n",
      "/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_small/I.pickle already present\n",
      "/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_small/J.pickle already present\n"
     ]
    }
   ],
   "source": [
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shivankurkapoor/Desktop/Study/ML/Udacity/Assignment1/notMNIST_large/A.pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10f9d6f50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x102f1c910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Testing if images are good\n",
    "pickle_file = train_datasets[0]\n",
    "print(pickle_file)\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    image_dataset = pickle.load(f)\n",
    "sample_image = image_dataset[2] * pixel_depth + pixel_depth / 2\n",
    "plt.imshow(sample_image, cmap='Greys', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "    if nb_rows:\n",
    "        dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "        labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "    else:\n",
    "        dataset, labels = None, None\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "    num_classes = len(pickle_files)\n",
    "    valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "    train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "    vsize_per_class = valid_size // num_classes\n",
    "    tsize_per_class = train_size // num_classes\n",
    "\n",
    "    start_v, start_t = 0, 0\n",
    "    end_v, end_t = vsize_per_class, tsize_per_class\n",
    "    end_l = vsize_per_class+tsize_per_class\n",
    "    for label, pickle_file in enumerate(pickle_files):       \n",
    "        try:\n",
    "              with open(pickle_file, 'rb') as f:\n",
    "                letter_set = pickle.load(f)\n",
    "                # let's shuffle the letters to have random validation and training set\n",
    "                np.random.shuffle(letter_set)\n",
    "                if valid_dataset is not None:\n",
    "                  valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "                  valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "                  valid_labels[start_v:end_v] = label\n",
    "                  start_v += vsize_per_class\n",
    "                  end_v += vsize_per_class\n",
    "\n",
    "                train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "                train_dataset[start_t:end_t, :, :] = train_letter\n",
    "                train_labels[start_t:end_t] = label\n",
    "                start_t += tsize_per_class\n",
    "                end_t += tsize_per_class\n",
    "        except Exception as e:\n",
    "            print('Unable to process data from', pickle_file, ':', e)\n",
    "            raise\n",
    "\n",
    "    return valid_dataset, valid_labels, train_dataset, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (200000, 28, 28) (200000,)\n",
      "Validation: (10000, 28, 28) (10000,)\n",
      "Testing: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation, :, :]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "try:\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        save = {\n",
    "            'train_dataset' : train_dataset,\n",
    "            'train_labels'  : train_labels,\n",
    "            'test_dataset'  : test_dataset,\n",
    "            'test_labels'   : test_labels,\n",
    "            'valid_dataset' : valid_dataset,\n",
    "            'valid_labels'  : valid_labels,\n",
    "        }\n",
    "        pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "except IOError as e:\n",
    "    print('Unable to save data to ', pickle_file, ':', e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 690800441\n"
     ]
    }
   ],
   "source": [
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg = linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(train_dataset.reshape((train_dataset.shape[0], -1)), train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82369999999999999"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_valid = logreg.predict(valid_dataset.reshape((valid_dataset.shape[0], -1)))\n",
    "logreg.score(valid_dataset.reshape((valid_dataset.shape[0], -1)), valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.83      0.85      1000\n",
      "          1       0.84      0.80      0.82      1000\n",
      "          2       0.82      0.87      0.85      1000\n",
      "          3       0.85      0.84      0.84      1000\n",
      "          4       0.83      0.75      0.79      1000\n",
      "          5       0.83      0.88      0.85      1000\n",
      "          6       0.83      0.83      0.83      1000\n",
      "          7       0.83      0.81      0.82      1000\n",
      "          8       0.75      0.79      0.77      1000\n",
      "          9       0.80      0.83      0.82      1000\n",
      "\n",
      "avg / total       0.82      0.82      0.82     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_labels, predict_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
